{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Batsal Ghimire\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e5d2038e11c57d2abd9d2c24d9421273",
     "grade": false,
     "grade_id": "cell-26967b3698cd6565",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Indexing Techniques and Data Structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "89b640ab4f7518099d2ca503738991ab",
     "grade": true,
     "grade_id": "cell-b0a98c487d3be44b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "## Counting Bloom Filters\n",
    "### Bloom Filters\n",
    "Bloom Filter is a very efficient data strucutre that can tell us if an item is present in a set or not. It has a constant time complexity for query and insertion, O(k), where 'k' is the number of hash functions. Bloom Filters also do not store any data, so it is incredibly memory efficient. Although it sounds like an ideal data strucutre, the price we pay for efficiency is that it is probabilistic in nature and we have a risk of getting a false positive.\n",
    "\n",
    "Bloom Filters can only be used to find if we've seen an item before, and it gives us no information about what data it contains.\n",
    "\n",
    "### How it works?\n",
    "Let's say that Bloom Filter uses 'm' bits to contain 'n' items using 'k' hash functions. We need to make sure that the hash functions are independent to one another, lack of which might lead to more false positives and clustering. It's necessary to strike a balance between the number of items to be stored and the number of hash functions. This is because more hash functions makes the Bloom Filter slow, but too few and false positive rate increases.\n",
    "\n",
    "Let's look at the diagram below to see how Bloom Filters Work.\n",
    "\n",
    "<img src=\"fig1.png\" alt=\"Fig1\" style=\"width: 700px;\"/>\n",
    "\n",
    "First, we create a memory size with 'm' bits. And, we pass each item through the hash function which gives us the index values. We flip these indexes to 1 or True. When we go to retrieve these elements, we pass the item through the hash functions and get the index values. And, if all indexes consist of 1's, we return that we've seen the element.\n",
    "<img src=\"fig2.png\" alt=\"Fig2\" style=\"width: 700px;\"/>\n",
    "\n",
    "If there are more than one elements with the same index, we don't update it to 2 and so on. It remains at 1. Here, we might observe the problem with this method. If we pass an element which is not in the data, but the index we get through the hashing function are all 1's, then we will still return that an element exists.\n",
    "\n",
    "<img src=\"fig3.png\" alt=\"Fig3\" style=\"width: 700px;\"/>\n",
    "\n",
    "However, there is no possibility for a false negative. It can never say that something does not exist, if it does. Since, we don't delete the 1's, once it is established in place, it stays there.\n",
    "\n",
    "Another problem with this is that it does not facilitate any way of deletion. If we delete the ones and replace it with zeros, then it might also affect the 1 implemented by another item. To solve this problem, there is an extension of bloom filters called 'Counting Bloom Filters.'\n",
    "\n",
    "### CBF's\n",
    "Counting Bloom filters also accept multiple bit counts, so we can also delete the items from the set. The array bits are incremented or decremented by 1 everytime we get the array position to be true. \n",
    "\n",
    "<img src=\"fig4.png\" alt=\"Fig4\" style=\"width: 700px;\"/>\n",
    "\n",
    "Since CBF's increase the count as more positions are True, we can decrement the counts by 1 to undo the increment caused by the item. This would be equivalent to deleting the item. This does not solve the problem of getting false positives, but it adds one more operation to the bloom filter. Also, we can change the threshold for the  CBF to only get True if the number of counts increases a certain point.\n",
    "\n",
    "It still maintains the constant time complexity of the Bloom Filters for all operations: insertion, query, and deletion.\n",
    "\n",
    "#### Selecting the optimal number of hash functions and memory size:\n",
    "The Wikipedia articles on Bloom Filters mention the equation given below, which I have used for the Python implementation.\n",
    "\n",
    "$$ k = \\frac{m}{n} log (2)$$\n",
    "where, 'k' is the number of hash functions, 'm' is the memory size, and 'n' is the number of elements that is to be inserted.\n",
    "\n",
    "There is another equation to find the  appropriate memory size for the filter which is given as:\n",
    "\n",
    "$$ m = \\frac{-n log(p)}{(log(2))^2} $$\n",
    "where, 'p' is the false positive probability.\n",
    "\n",
    "From this equation, we can observe that we can change the false positive rate, depending on the use scenario and the room for error present.\n",
    "\n",
    "### Applications\n",
    "Counting Bloomm Filter's can be used in many areas, but we need to make sure that the consequence for a false positive are not very high.\n",
    "\n",
    "One application can be to check if an important item exists before cleaning a drive or some storage device. It would guarantee to void the process if the item is still present and notify the user.\n",
    "\n",
    "It can also help in the process of finding unique SSN, specially when there are millions of unique IDs already. We can generate a random number and check if that number already exists. Since, there is no chance of getting a false negative, getting a False would mean that the ID has never been used before.\n",
    "\n",
    "Recommending videos on YouTube and Netflix, finding unique URLs, etc. can also make use of CBFs.\n",
    "\n",
    "However, it should not be used in big missions where the missing of one file can be devastating (like space missions). If we ask a CBF to see if a file exists, it might return a true even if the file does not exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "txt_file = open(\"t8.shakespeare.txt\", \"r\")\n",
    "\n",
    "entries = txt_file.read().split(' ')\n",
    "lines = [string.replace('\\n', '') for string in entries]\n",
    "all_text = [line for line in lines if line != '']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Hashing\n",
    "We will be using mmh3 to generate the hash functions since it is very fast and we can make it very secure by changing the parameters.\n",
    "\n",
    "mmh3 takes in two arguments. The first is the element which we intend to hash, and the second is the seed, if we choose to use it. In this implementation, I will be using the seeds which will be generated by the random module in python. This is the main reason I chose mmh3, since it can provide us with independent hash functions by just changing the seeds.\n",
    "\n",
    "The hash function must be deterministic. So, the seed method in the random module will output a pseudo random number. This means that using the same seed should output the same number which maintains the deterministic nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Threshold\n",
    "This implementation uses threshold, but it is not necessary since we can just return True if there is a non-zero value in the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import math\n",
    "import random\n",
    "import hashlib\n",
    "import mmh3\n",
    "# Feel free to define additional classes that you think are helpful \n",
    "# in building this class of CountingBloomFilter \n",
    "\n",
    "class CountingBloomFilter(object):\n",
    "    \"\"\"Implement the counting bloom filter which supports:\n",
    "    - search: queries the membership of an element\n",
    "    - insert: inserts a string to the filter\n",
    "    - delete: removes a string from the filter \n",
    "    \n",
    "    Feel free to define any helpful additional methods.\n",
    "    \"\"\"\n",
    "    #We pass in the false positive rate and the number of items that needs be stored as the parameter. This is because the optimal number of hash functions and the array size can be computed by using the formula if we  have these parameters.\n",
    "    def __init__(self, fpr, num_item, seed, threshold):\n",
    "        \"\"\"\n",
    "        /YOUR ARGUMENTS/ are the two parameters of your choice from the \n",
    "        following parameters of a CBF:\n",
    "        - fpr: float, false positive rate\n",
    "        - memory_size: int, memory size\n",
    "        - num_item: int, number of items stored\n",
    "        - num_hashfn: int, number of hash functions\n",
    "        \n",
    "        For example, if you choose fpr and memory_size, edit your __init__ to\n",
    "        `def __init__(self, memory_size, fpr)`\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.seed = seed\n",
    "        self.num_item = num_item\n",
    "        self.fpr = fpr #I'm assuming that  fpr is given as a probability. If not, we can easily convert percentage into probability.\n",
    "        \n",
    "        #We will use the formula that we found above to find the size of the array and the number of hash functions.\n",
    "        #First for memory_size, because we need it to find the num_hashfn.\n",
    "        #Formula: m = -n ln(P)/(ln2)^2\n",
    "        self.memory_size = abs(int(round(-self.num_item * math.log(self.fpr))/(math.log(2)**2))) #For this we need the math library\n",
    "    \n",
    "        #Now, for number of hash functions\n",
    "        #Formula: k = (m/n)ln(2)\n",
    "        self.num_hashfn = abs(int(round((self.memory_size/self.num_item)* math.log(2))))\n",
    "        \n",
    "        #We have the size of the list, so we can create the list and initialize it with all zeros.\n",
    "        self.slots = self.memory_size * [0]\n",
    "        \n",
    "    def hash_cbf(self, item):\n",
    "        \"\"\"\n",
    "        Returns hash values of an item\n",
    "        [ADD ADDITIONAL DESCRIPTION, IF NEED BE]\n",
    "        \"\"\"\n",
    "        #We use the mmh3 hash to get the hash function. mmh3 has two arguments, the first is the item which is fixed, but we differe the seed.\n",
    "        for i in self.seed[:self.num_hashfn]:\n",
    "            self.hash_val = mmh3.hash(item, i) % self.memory_size\n",
    "        return self\n",
    "    \n",
    "    def search(self, item):\n",
    "        \"\"\"\n",
    "        Searches for the item\n",
    "        \"\"\"\n",
    "        #Seed should give us the same values as long as the initial state are the same.\n",
    "        #We check if the slot value is greater than the threshold. For example: if a slot has value '2', then it compares it with the threshold we have set and return true if it is greater.\n",
    "        for seed_val in self.seed[:self.num_hashfn]:\n",
    "            if (self.slots[mmh3.hash(item, seed_val)%self.memory_size]) > self.threshold: #For this Assignment, I have set all the threshold to be 0.\n",
    "                pass\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def insert(self, item):\n",
    "        \"\"\"\n",
    "        Insert an item\n",
    "        \"\"\"\n",
    "        for seed_val in self.seed[:self.num_hashfn]:\n",
    "            #We get the index from the hash function, and we increase that position's value by 1.\n",
    "            self.slots[mmh3.hash(item, seed_val)%self.memory_size] = (self.slots[mmh3.hash(item, seed_val)%self.memory_size]) + 1\n",
    "        return self\n",
    "    \n",
    "    def delete(self, item):\n",
    "        \"\"\"\n",
    "        Deletes an item\n",
    "        \"\"\"\n",
    "        #Searches if the item exists. If not, there is no way to delete it.\n",
    "        if self.search(item) == True:\n",
    "            for seed_val in self.seed[:self.num_hashfn]:\n",
    "                #Similar to when inserting, the value is incremented by 1. While deleting, we decrease this value by 1.\n",
    "                self.slots[mmh3.hash(item, seed_val)%self.memory_size] = (self.slots[mmh3.hash(item, seed_val)%self.memory_size]) - 1\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joker\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is a simple test used to demonstrate how the implementation works.\n",
    "l = ['joker'] #We add one element to the list.\n",
    "seed_list = [3,4,5,6,7,1] #Set the seeds.\n",
    "cbf = CountingBloomFilter(0.4,2, seed_list,0) #Create an object with the given parameters\n",
    "for i in l: # Goes through the list 'l'.\n",
    "    print (i)\n",
    "    cbf.insert(i) #Inserts the value in the set\n",
    "cbf.search('joker') #Searches for joker, and we it returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 't8.shakespeare.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bdccb380a5f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Given in the assignment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtxt_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t8.shakespeare.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtxt_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 't8.shakespeare.txt'"
     ]
    }
   ],
   "source": [
    "#Given in the assignment.\n",
    "txt_file = open(\"t8.shakespeare.txt\", \"r\")\n",
    "\n",
    "entries = txt_file.read().split(' ')\n",
    "lines = [string.replace('\\n', '') for string in entries]\n",
    "all_text = [line for line in lines if line != '']\n",
    "count = 0\n",
    "l = []\n",
    "#If we see that the implementation returns false, even if the element exists, it is because words with punctuation act as a single item.\n",
    "\"\"\"These are the parameters we can adjust to get different results.\"\"\"\n",
    "\n",
    "#In the text file, in which word do you want the search to begin. \n",
    "start_word = 1826\n",
    "\n",
    "#In the text file, in which word do you want the search to end. \n",
    "end_word = 1930\n",
    "\n",
    "#Set the probability of getting a false positive\n",
    "false_positive_prob = 0.2\n",
    "\n",
    "#Set the number of seeds you want to use\n",
    "num_seeds = 100\n",
    "\n",
    "#Set the threshold you want to use.\n",
    "threshold = 0\n",
    "\n",
    "#What word would you like to search for,\n",
    "search_word = 'give?'\n",
    "\n",
    "#Which word would you like to delete.\n",
    "del_word = ''\n",
    "\n",
    "\"\"\"These are the parameters we can adjust to get different results.\"\"\"\n",
    "\n",
    "#This is just an unnecessary addition. Since the runtime was very high when I used the entire dataset, I implemented a way to select the start and end position of the word.\n",
    "for i in range(start_word,end_word):\n",
    "    l.append(all_text[i]) #Creates a list depending on the start and end values we enter\n",
    "    count = count + 1\n",
    "    \n",
    "seeds = [] #Creates a set for the seeds.\n",
    "\n",
    "for i in range(num_seeds): \n",
    "    seeds.append(random.randrange(0,100)) #Creates a list of seeds that we need to feed while creating the object\n",
    "cbf = CountingBloomFilter(false_positive_prob,count,seeds,threshold) #We can change all these parameters.\n",
    "\n",
    "for j in l: #Inserts each element from the list 'l' to the set.\n",
    "    cbf.insert(j)\n",
    "    \n",
    "if del_word:\n",
    "    if (cbf.search(del_word)) == True: #If it finds the word\n",
    "        print (\"Word was found.\")\n",
    "        print (\"Deleting the word\")\n",
    "        cbf.delete(del_word) #Deletes the word\n",
    "        if cbf.search(del_word) == False: #If the item is deleted and the search comes out as False\n",
    "            print (\"Your word '\", del_word, \"' is deleted.\")\n",
    "            print (\"---------------------------------------\")\n",
    "        else: #If the item is deleted but the search comes out as True i.e. a false positive.\n",
    "            print (\"Something's not right, I don't feel too good.\")\n",
    "            print (\"---------------------------------------------\")\n",
    "    else: #If the word does not exist in the first place\n",
    "        print (\"The word was not found.\")\n",
    "        print (\"------------------------\")\n",
    "        \n",
    "if search_word: #Searches for the word.\n",
    "    if (cbf.search(search_word)) == True: #If it exists\n",
    "         print (\"The word '\" , search_word , \"'exists within the given range of words :)\")\n",
    "    else: #If  it does not exist.\n",
    "         print (\"The word '\" , search_word , \"'cannot be found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f454ec05ad12849a094bb2017f596b7c",
     "grade": true,
     "grade_id": "cell-a1db8a260a21986a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "### 1. Memory size and FPR\n",
    "Theoretically, if we look at the equation, we can see that the relation between memory size(m) and FPR (probability) is inverse logarithmic. The graph we got also looks like the flipped version of the log graph. This shows that as the size of the array increases, the false positive rate decreases. This is due to the lessening number of collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a random word generator\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Adapted from: https://pynative.com/python-generate-random-string/\n",
    "#The above given article implements a way to easily create a list of random words for testing.\n",
    "def random_word(length):\n",
    "    let = string.ascii_lowercase\n",
    "    return ''.join(random.choice(let) for i in range(length))\n",
    "\n",
    "false_positive = [] #Since, we are looking at the relation between memory size and FPR, we change the value of FPR.\n",
    "for i in range(1,101):\n",
    "    false_positive.append(i/100) #Change the percentage to probability, since this is what our class takes\n",
    "false_positive_percentage = [round(100*j) for j in false_positive] #This is only used for plotting\n",
    "\n",
    "\n",
    "num_item = 100 #Number of items we want to enter\n",
    "items = [random_word(10) for words in range(num_item)] #Creates the given number of random words of length 10.\n",
    "memory_size = [] #Sets the list  for memory size, which we use for plotting.\n",
    "for k in false_positive: #Creates objects with varying FPR's\n",
    "    cbf = CountingBloomFilter(k, num_item, seeds, 0)\n",
    "    for i in items:\n",
    "        cbf.insert(i) #Inserts the item\n",
    "    memory_size.append(cbf.memory_size*sys.getsizeof(1)) #I found a way to find the memory_size with the help of https://stackoverflow.com/questions/449560/how-do-i-determine-the-size-of-an-object-in-python\n",
    "\n",
    "#Rest of the segment is for plotting purposes.\n",
    "plt.plot(false_positive_percentage, memory_size)\n",
    "plt.xlabel(\"False Positive Rate (FPR) in percentage\")\n",
    "plt.ylabel(\"Memory size\")\n",
    "plt.grid(True)\n",
    "plt.title(\"How memory size scales with FPR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory size and number of items\n",
    "Again, like the linear relation between m and n in the equation, our graph is also linear. As more elements are added, the memory size also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A lot is similar between the previous plot and this one. I will only comment the ones that are different.\n",
    "num_item = 1000\n",
    "items = [random_word(10) for words in range(num_item)]\n",
    "memory_size = []\n",
    "nums= 0\n",
    "list_inputs = []\n",
    "for num in range(1, num_item): #Changes the  number of items in the list\n",
    "    n = items[:num] #Slices the list to get the items within the range\n",
    "    cbf = CountingBloomFilter(0.1, len(n), seeds, 0)\n",
    "    nums = nums + 1\n",
    "    list_inputs.append(nums)\n",
    "    for i in n:\n",
    "        cbf.insert(i)\n",
    "    memory_size.append(cbf.memory_size * sys.getsizeof(1))\n",
    "plt.plot(list_inputs, memory_size)\n",
    "plt.xlabel(\"Number of items\")\n",
    "plt.ylabel(\"Memory size\")\n",
    "plt.grid(True)\n",
    "plt.title(\"How memory size scales with number of items\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPR and the number of hash functions\n",
    "As we have more independent hash functions, less collision occurs because there are more conditions to be fulfilled before we return the presence or absence of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theoretical rate\n",
    "false_positive = []\n",
    "for i in range(1,101):\n",
    "    false_positive.append(i/100)\n",
    "false_positive_percentage = [round(100*j) for j in false_positive]\n",
    "list_of_hashfn = []\n",
    "\n",
    "for j in false_positive:\n",
    "    cbf = CountingBloomFilter(j, len(items), seeds, 0)\n",
    "    list_of_hashfn.append(cbf.num_hashfn)\n",
    "\n",
    "#The way I do this is by creating two lists with random words. Then, I insert the items from the first list into the set. And, then I search for item that are not contained in the list 1 but still return as True i.e. False Positive.\n",
    "list1 = [random_word(10) for words in range(500)]\n",
    "list2 = [random_word(10) for words in range(500)]\n",
    "li_fpr = []\n",
    "t_hashfn = []\n",
    "\n",
    "\n",
    "for k in false_positive:\n",
    "    cbf = CountingBloomFilter(k, len(list1), seeds, 0)\n",
    "    t_hashfn.append(cbf.num_hashfn)\n",
    "    for li in list1:\n",
    "        cbf.insert(li)\n",
    "    n_false_positives = 0\n",
    "    for v in list2:\n",
    "        cbf.search(v)\n",
    "        if cbf.search(v) == True and (v not in list1):\n",
    "            n_false_positives += 1\n",
    "    li_fpr.append(n_false_positives/len(list2))            \n",
    "    \n",
    "plt.plot(list_of_hashfn, false_positive)\n",
    "plt.xlabel(\"Number of hash functions\")\n",
    "plt.ylabel(\"False positive probability\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Theoretical comparison\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t_hashfn, li_fpr)\n",
    "plt.xlabel(\"Number of hash functions\")\n",
    "plt.ylabel(\"Fa\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Practical comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access time and number of items\n",
    "How many hash functions we use should not depend on the number of items to be stored. So, the access time is relatively constant with plenty of noise in both directions. Searching for some elements might take longer than other, which might explain this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have used the time module to calculate the time and statistics to make it easy to find the average.\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "access_time = []\n",
    "fpr = 0.05 #Set the FPR to be fixed\n",
    "list_ = [random_word(10) for words in range(1000)]\n",
    "size = []\n",
    "for i in range(1, len(list_)):\n",
    "    slic = list_[:i] #Slice the list like we did before\n",
    "    cbf = CountingBloomFilter(fpr, len(slic), seeds, 0)\n",
    "    for j in slic:\n",
    "        cbf.insert(j)\n",
    "        avg_time = []\n",
    "    size.append(i)\n",
    "        \n",
    "    for i in range(10): #How many iterations we want to get more robust results.\n",
    "        time_ = [] \n",
    "           \n",
    "        for j in slic:\n",
    "            begin_time = time.time()\n",
    "            cbf.search(j)\n",
    "            end_time = time.time()\n",
    "            time_.append(end_time-begin_time)\n",
    "        avg_time.append(statistics.mean(time_))\n",
    "    access_time.append(statistics.mean(avg_time)) \n",
    "\n",
    "plt.plot(size, access_time)\n",
    "plt.xlabel(\"Number of items\")\n",
    "plt.ylabel(\"Time to search for items\")\n",
    "plt.title(\"Relation between access time and number of items\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
